{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFj/kwNa2N46bH8DCro6LQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/petr0vsk/Fraud/blob/main/Fraud_clusters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Считаем процентили 90 для всех ЮЛ"
      ],
      "metadata": {
        "id": "SvutK3d7fKtH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Метод k-means"
      ],
      "metadata": {
        "id": "o-GWtcmGz6IC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# 1. Загрузка и подготовка данных\n",
        "\n",
        "def load_and_process_csv(csv_file):\n",
        "    \"\"\"\n",
        "    Загружает данные из CSV, проверяет их корректность и выполняет необходимые преобразования.\n",
        "\n",
        "    Аргументы:\n",
        "        csv_file (str): путь к CSV-файлу.\n",
        "\n",
        "    Возвращает:\n",
        "        pd.DataFrame: обработанный датафрейм с рассчитанными процентилями и оборотами.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(csv_file, delimiter=';', header=None)\n",
        "    df.columns = ['inn', 'org_name', 'amount', 'type', 'code', 'timestamp']\n",
        "\n",
        "    # Преобразование суммы в числовой формат\n",
        "    df['amount'] = df['amount'].apply(lambda x: str(x).replace(',', '.'))\n",
        "    df['amount'] = pd.to_numeric(df['amount'], errors='coerce')\n",
        "\n",
        "    # Группировка по INN и расчет percentil 90 и total_payments\n",
        "    result = df.groupby(['inn', 'org_name']).agg(\n",
        "        percentile_90=('amount', lambda x: x.quantile(0.9)),\n",
        "        total_payments=('amount', 'sum')\n",
        "    ).reset_index()\n",
        "\n",
        "    return df, result  # Возвращаем и исходный DataFrame для расчетов ложных срабатываний\n",
        "\n",
        "# 2. Кластеризация с тюнингом параметров\n",
        "\n",
        "def cluster_businesses_with_tuning(df, n_clusters=3, init='k-means++', n_init=10, max_iter=300):\n",
        "    \"\"\"\n",
        "    Выполняет кластеризацию с тюнингом параметров.\n",
        "\n",
        "    Аргументы:\n",
        "        df (pd.DataFrame): датафрейм с данными для кластеризации.\n",
        "        n_clusters (int): количество кластеров.\n",
        "        init (str): метод инициализации центроидов ('k-means++' или 'random').\n",
        "        n_init (int): количество запусков алгоритма.\n",
        "        max_iter (int): максимальное количество итераций.\n",
        "\n",
        "    Возвращает:\n",
        "        список датафреймов: датафреймы для каждого кластера.\n",
        "    \"\"\"\n",
        "    clustering_data = df[['percentile_90', 'total_payments']]\n",
        "    kmeans = KMeans(n_clusters=n_clusters, init=init, n_init=n_init, max_iter=max_iter, random_state=0)\n",
        "    df['n_cluster'] = kmeans.fit_predict(clustering_data)\n",
        "\n",
        "    clusters = []\n",
        "    for i in range(n_clusters):\n",
        "        clusters.append(df[df['n_cluster'] == i].copy())\n",
        "\n",
        "    return clusters\n",
        "\n",
        "# 3. Оценка модели с использованием силуэтного коэффициента\n",
        "\n",
        "def evaluate_clustering_with_silhouette(df_list):\n",
        "    \"\"\"\n",
        "    Оценивает качество кластеризации с помощью силуэтного коэффициента.\n",
        "\n",
        "    Аргументы:\n",
        "        df_list (list): список датафреймов для каждого кластера.\n",
        "\n",
        "    Возвращает:\n",
        "        float: силуэтный коэффициент.\n",
        "    \"\"\"\n",
        "    combined_df = pd.concat(df_list)\n",
        "    X = combined_df[['percentile_90', 'total_payments']]\n",
        "    labels = combined_df['n_cluster']\n",
        "\n",
        "    score = silhouette_score(X, labels)\n",
        "    return score\n",
        "\n",
        "# 4. Расчет порогов срабатывания и сохранение результатов\n",
        "\n",
        "def calculate_and_save_thresholds_with_names(clustered_dfs):\n",
        "    \"\"\"\n",
        "    Вычисляет пороги для каждого кластера и сохраняет результаты.\n",
        "\n",
        "    Аргументы:\n",
        "        clustered_dfs (list): список датафреймов для каждого кластера.\n",
        "\n",
        "    Возвращает:\n",
        "        dict: словарь с порогами для каждого кластера.\n",
        "    \"\"\"\n",
        "    thresholds = {}\n",
        "\n",
        "    for i, cluster_df in enumerate(clustered_dfs):\n",
        "        threshold_value = cluster_df['percentile_90'].quantile(0.9)\n",
        "        cluster_name = f\"cluster_{i}\"  # Автоматическая генерация имени кластера\n",
        "        thresholds[cluster_name] = threshold_value\n",
        "\n",
        "        cluster_df['threshold'] = threshold_value\n",
        "\n",
        "        #print(f\"Порог для {cluster_name}: {threshold_value}\")\n",
        "\n",
        "    return thresholds\n",
        "\n",
        "# 5. Расчет ложных срабатываний\n",
        "\n",
        "def calculate_false_triggers(df_transactions, df_cluster, threshold):\n",
        "    \"\"\"\n",
        "    Рассчитывает количество ложных срабатываний для каждого ИНН в кластере.\n",
        "\n",
        "    Аргументы:\n",
        "        df_transactions (pd.DataFrame): датафрейм с транзакциями.\n",
        "        df_cluster (pd.DataFrame): датафрейм с данными кластера.\n",
        "        threshold (float): порог для данного кластера.\n",
        "\n",
        "    Возвращает:\n",
        "        pd.DataFrame: датафрейм с добавленным столбцом 'errors'.\n",
        "    \"\"\"\n",
        "    # Подсчитываем количество транзакций по каждому ИНН\n",
        "    transaction_counts = df_transactions.groupby('inn').agg(total_transactions=('amount', 'count')).reset_index()\n",
        "\n",
        "    merged_df = pd.merge(df_transactions, df_cluster[['inn', 'threshold']], on='inn', how='left')\n",
        "\n",
        "    # Рассчитываем ложные срабатывания\n",
        "    merged_df['errors'] = merged_df['amount'] > threshold\n",
        "\n",
        "    # Считаем количество ложных срабатываний по каждому ИНН\n",
        "    false_triggers_df = merged_df.groupby('inn').agg(total_errors=('errors', 'sum')).reset_index()\n",
        "\n",
        "    # Объединяем обратно с кластером и добавляем количество транзакций\n",
        "    df_cluster = pd.merge(df_cluster, false_triggers_df[['inn', 'total_errors']], on='inn', how='left')\n",
        "    df_cluster = pd.merge(df_cluster, transaction_counts, on='inn', how='left')\n",
        "    eps = 0.05  # значение можно настроить в зависимости от данных\n",
        "    min_samples = 10\n",
        "    #print(f\"Количество ложных срабатываний и транзакций добавлено для кластера: {df_cluster[['inn', 'total_errors', 'total_transactions']].head()}\")\n",
        "\n",
        "    return df_cluster\n",
        "\n",
        "# 6. Вывод метрик по кластерам\n",
        "\n",
        "def print_cluster_metrics(all_clusters_df):\n",
        "    \"\"\"\n",
        "    Выводит метрики по кластерам, включая процент ложных срабатываний по каждому кластеру и общий процент.\n",
        "\n",
        "    Аргументы:\n",
        "        all_clusters_df (pd.DataFrame): объединенный датафрейм с данными всех кластеров.\n",
        "    \"\"\"\n",
        "    # Подсчет общего количества транзакций и ложных срабатываний по кластерам\n",
        "    cluster_metrics = all_clusters_df.groupby('n_cluster').agg(\n",
        "        total_transactions=('total_transactions', 'sum'),\n",
        "        total_errors=('total_errors', 'sum')\n",
        "    ).reset_index()\n",
        "\n",
        "    total_errors_all_clusters = cluster_metrics['total_errors'].sum()\n",
        "    total_transactions_all_clusters = cluster_metrics['total_transactions'].sum()\n",
        "    overall_error_rate = (total_errors_all_clusters / total_transactions_all_clusters) * 100 if total_transactions_all_clusters > 0 else 0\n",
        "\n",
        "    # Вывод метрик по кластерам\n",
        "    for _, row in cluster_metrics.iterrows():\n",
        "        error_rate = (row['total_errors'] / row['total_transactions']) * 100 if row['total_transactions'] > 0 else 0\n",
        "        print(f\"Кластер {int(row['n_cluster'])}:\")\n",
        "        print(f\"  Всего транзакций: {row['total_transactions']}\")\n",
        "        print(f\"  Ложных срабатываний: {row['total_errors']}\")\n",
        "        print(f\"  Процент ложных срабатываний: {error_rate:.2f}%\")\n",
        "\n",
        "    # Вывод общего процента ложных срабатываний по всем кластерам\n",
        "    print(f\"Общий процент ложных срабатываний по всем кластерам: {overall_error_rate:.2f}%\")\n",
        "\n",
        "\n",
        "# 7. Основная часть кода\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Загрузка данных\n",
        "    file_path = 'fraud_02.2024.csv'\n",
        "    df_transactions, df_aggregated = load_and_process_csv(file_path)\n",
        "\n",
        "    # Кластеризация на агрегированных данных\n",
        "    num_clusters = 3  # Задаем произвольное количество кластеров\n",
        "    clustered_dfs = cluster_businesses_with_tuning(df_aggregated, n_clusters=num_clusters, init='k-means++', n_init=20, max_iter=500)\n",
        "\n",
        "    # Оценка силуэтного коэффициента\n",
        "    silhouette_score_value = evaluate_clustering_with_silhouette(clustered_dfs)\n",
        "    print(f\"Силуэтный коэффициент: {silhouette_score_value}\")\n",
        "\n",
        "    # Расчет порогов для каждого кластера\n",
        "    thresholds = calculate_and_save_thresholds_with_names(clustered_dfs)\n",
        "\n",
        "    # Инициализируем список для хранения данных по каждому кластеру\n",
        "    cluster_info = []\n",
        "    total_transactions_all_clusters = 0\n",
        "    total_errors_all_clusters = 0\n",
        "\n",
        "    # Рассчитываем ложные срабатывания и добавляем общее количество транзакций для каждого кластера\n",
        "    all_clusters_df = pd.DataFrame()  # Общая таблица для всех кластеров\n",
        "    for i, (cluster_name, threshold) in enumerate(thresholds.items()):\n",
        "        df_cluster = clustered_dfs[i]\n",
        "\n",
        "        # Рассчитываем ложные срабатывания\n",
        "        df_cluster_with_errors = calculate_false_triggers(df_transactions, df_cluster, threshold)\n",
        "\n",
        "        # Добавляем к общему DataFrame\n",
        "        all_clusters_df = pd.concat([all_clusters_df, df_cluster_with_errors], ignore_index=True)\n",
        "\n",
        "        # Собираем информацию для таблицы\n",
        "        total_transactions = df_cluster_with_errors['total_transactions'].sum()\n",
        "        total_errors = df_cluster_with_errors['total_errors'].sum()\n",
        "        error_rate = (total_errors / total_transactions) * 100 if total_transactions > 0 else 0\n",
        "        num_rows = df_cluster_with_errors.shape[0]\n",
        "\n",
        "        # Обновляем общий счетчик транзакций и ошибок\n",
        "        total_transactions_all_clusters += total_transactions\n",
        "        total_errors_all_clusters += total_errors\n",
        "\n",
        "        cluster_info.append({\n",
        "            'Кластер': cluster_name,\n",
        "            'Перцентиль 90%': int(round(df_cluster['percentile_90'].quantile(0.9))),\n",
        "            'Порог срабатывания': int(round(threshold)),\n",
        "            'Всего транзакций': total_transactions,\n",
        "            'Ложных срабатываний': total_errors,\n",
        "            'Процент ложных срабатываний': f\"{error_rate:.2f}%\",\n",
        "            'Кол-во строк': num_rows\n",
        "        })\n",
        "\n",
        "    # Преобразуем собранные данные в DataFrame для вывода в виде таблицы\n",
        "    cluster_info_df = pd.DataFrame(cluster_info)\n",
        "\n",
        "    # Вывод таблицы\n",
        "    print(\"\\nИнформация по кластерам:\")\n",
        "    print(cluster_info_df.to_string(index=False))\n",
        "\n",
        "    # Рассчитываем и выводим общий процент ложных срабатываний по всем кластерам\n",
        "    overall_error_rate = (total_errors_all_clusters / total_transactions_all_clusters) * 100 if total_transactions_all_clusters > 0 else 0\n",
        "    print(f\"\\nОбщий процент ложных срабатываний по всем кластерам: {overall_error_rate:.2f}%\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qlhu0xRM8iT2",
        "outputId": "cd47d7e4-160d-4d04-f92d-d9175045c3f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Силуэтный коэффициент: 0.9274107872732689\n",
            "\n",
            "Информация по кластерам:\n",
            "  Кластер  Перцентиль 90%  Порог срабатывания  Всего транзакций  Ложных срабатываний Процент ложных срабатываний  Кол-во строк\n",
            "cluster_0         1000000             1000000             66749                 2149                       3.22%          2208\n",
            "cluster_1        13686500            13686500              1401                   32                       2.28%             4\n",
            "cluster_2        11610100            11610100             18895                  139                       0.74%            71\n",
            "\n",
            "Общий процент ложных срабатываний по всем кластерам: 2.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Используем метод DBSCAN"
      ],
      "metadata": {
        "id": "pnNU7RXwOOfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import davies_bouldin_score\n",
        "\n",
        "# 1. Загрузка и подготовка данных\n",
        "\n",
        "def load_and_process_csv(csv_file):\n",
        "    \"\"\"\n",
        "    Загружает данные из CSV, проверяет их корректность и выполняет необходимые преобразования.\n",
        "\n",
        "    Аргументы:\n",
        "        csv_file (str): путь к CSV-файлу.\n",
        "\n",
        "    Возвращает:\n",
        "        pd.DataFrame: обработанный датафрейм с рассчитанными процентилями и оборотами.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(csv_file, delimiter=';', header=None)\n",
        "    df.columns = ['inn', 'org_name', 'amount', 'type', 'code', 'timestamp']\n",
        "\n",
        "    # Преобразование суммы в числовой формат\n",
        "    df['amount'] = df['amount'].apply(lambda x: str(x).replace(',', '.'))\n",
        "    df['amount'] = pd.to_numeric(df['amount'], errors='coerce')\n",
        "\n",
        "    # Группировка по INN и расчет percentil 90 и total_payments\n",
        "    result = df.groupby(['inn', 'org_name']).agg(\n",
        "        percentile_90=('amount', lambda x: x.quantile(0.9)),\n",
        "        total_payments=('amount', 'sum')\n",
        "    ).reset_index()\n",
        "\n",
        "    return df, result  # Возвращаем и исходный DataFrame для расчетов ложных срабатываний\n",
        "\n",
        "# 2. Кластеризация с использованием DBSCAN\n",
        "\n",
        "def cluster_businesses_with_dbscan(df, eps=0.5, min_samples=5):\n",
        "    \"\"\"\n",
        "    Выполняет кластеризацию с использованием DBSCAN.\n",
        "\n",
        "    Аргументы:\n",
        "        df (pd.DataFrame): датафрейм с данными для кластеризации.\n",
        "        eps (float): максимальное расстояние между точками для образования кластера.\n",
        "        min_samples (int): минимальное количество точек для формирования кластера.\n",
        "\n",
        "    Возвращает:\n",
        "        список датафреймов: датафреймы для каждого кластера.\n",
        "    \"\"\"\n",
        "    clustering_data = df[['percentile_90', 'total_payments']].values\n",
        "    db = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "    df['n_cluster'] = db.fit_predict(clustering_data)\n",
        "\n",
        "    clusters = []\n",
        "    unique_clusters = df['n_cluster'].unique()\n",
        "\n",
        "    for cluster in unique_clusters:\n",
        "        clusters.append(df[df['n_cluster'] == cluster].copy())\n",
        "\n",
        "    return clusters, df['n_cluster']\n",
        "\n",
        "# 3. Оценка модели с использованием Davies-Bouldin Index\n",
        "\n",
        "def evaluate_clustering_with_davies_bouldin(df, cluster_labels):\n",
        "    \"\"\"\n",
        "    Оценивает качество кластеризации с помощью индекса Davies-Bouldin.\n",
        "\n",
        "    Аргументы:\n",
        "        df (pd.DataFrame): датафрейм с данными для оценки.\n",
        "        cluster_labels (array): метки кластеров.\n",
        "\n",
        "    Возвращает:\n",
        "        float: индекс Davies-Bouldin.\n",
        "    \"\"\"\n",
        "    X = df[['percentile_90', 'total_payments']].values\n",
        "    if len(set(cluster_labels)) > 1:\n",
        "        score = davies_bouldin_score(X, cluster_labels)\n",
        "    else:\n",
        "        score = float('inf')  # Если кластер один или все -1 (шум), Davies-Bouldin не может быть рассчитан\n",
        "    return score\n",
        "\n",
        "# 4. Расчет порогов срабатывания и сохранение результатов\n",
        "\n",
        "def calculate_and_save_thresholds_with_names(clustered_dfs, cluster_names):\n",
        "    \"\"\"\n",
        "    Вычисляет пороги для каждого кластера и сохраняет результаты.\n",
        "\n",
        "    Аргументы:\n",
        "        clustered_dfs (list): список датафреймов для каждого кластера.\n",
        "        cluster_names (list): имена для каждого кластера (например, ['small', 'medium', 'big']).\n",
        "\n",
        "    Возвращает:\n",
        "        dict: словарь с порогами для каждого кластера.\n",
        "    \"\"\"\n",
        "    thresholds = {}\n",
        "\n",
        "    for i, cluster_df in enumerate(clustered_dfs):\n",
        "        threshold_value = cluster_df['percentile_90'].quantile(0.9)\n",
        "        thresholds[cluster_names[i]] = threshold_value\n",
        "\n",
        "        cluster_df['threshold'] = threshold_value\n",
        "\n",
        "    return thresholds\n",
        "\n",
        "# 5. Расчет ложных срабатываний\n",
        "\n",
        "def calculate_false_triggers(df_transactions, df_cluster, threshold):\n",
        "    \"\"\"\n",
        "    Рассчитывает количество ложных срабатываний для каждого ИНН в кластере и добавляет общий счетчик транзакций.\n",
        "\n",
        "    Аргументы:\n",
        "        df_transactions (pd.DataFrame): датафрейм с транзакциями.\n",
        "        df_cluster (pd.DataFrame): датафрейм с данными кластера.\n",
        "        threshold (float): порог для данного кластера.\n",
        "\n",
        "    Возвращает:\n",
        "        pd.DataFrame: датафрейм с добавленными столбцами 'total_errors' и 'total_trans'.\n",
        "    \"\"\"\n",
        "    # Подсчет количества транзакций для каждого ИНН\n",
        "    trans_counts = df_transactions.groupby('inn').agg(total_trans=('amount', 'count')).reset_index()\n",
        "\n",
        "    merged_df = pd.merge(df_transactions, df_cluster[['inn', 'threshold']], on='inn', how='left')\n",
        "\n",
        "    # Рассчет ошибок на основе транзакций\n",
        "    merged_df['errors'] = merged_df['amount'] > threshold\n",
        "    false_triggers_df = merged_df.groupby('inn').agg(total_errors=('errors', 'sum')).reset_index()\n",
        "\n",
        "    # Объединяем обратно с кластером\n",
        "    df_cluster = pd.merge(df_cluster, false_triggers_df[['inn', 'total_errors']], on='inn', how='left')\n",
        "    df_cluster = pd.merge(df_cluster, trans_counts[['inn', 'total_trans']], on='inn', how='left')\n",
        "\n",
        "    # Заполняем возможные пропуски (NaN) нулями\n",
        "    df_cluster['total_errors'] = df_cluster['total_errors'].fillna(0)\n",
        "    df_cluster['total_trans'] = df_cluster['total_trans'].fillna(0)\n",
        "\n",
        "    return df_cluster\n",
        "\n",
        "\n",
        "# 6. Основная часть кода\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Загрузка данных\n",
        "    file_path = 'fraud_02.2024.csv'\n",
        "    df_transactions, df_aggregated = load_and_process_csv(file_path)\n",
        "min_cluster_size = 5  #\n",
        "    # Кластеризация на агрегированных данных с использованием DBSCAN\n",
        "    eps = 0.01  # значение можно настроить в зависимости от данных\n",
        "    min_samples = 5\n",
        "    clustered_dfs, cluster_labels = cluster_businesses_with_dbscan(df_aggregated, eps=eps, min_samples=min_samples)\n",
        "\n",
        "    # Оценка индекса Davies-Bouldin\n",
        "    davies_bouldin_score_value = evaluate_clustering_with_davies_bouldin(df_aggregated, cluster_labels)\n",
        "    print(f\"Индекс Davies-Bouldin: {davies_bouldin_score_value}\")\n",
        "\n",
        "    # Определение имен для каждого кластера (кроме шума)\n",
        "    cluster_names = [f\"cluster_{i}\" for i in range(len(clustered_dfs))]\n",
        "\n",
        "    # Расчет порогов для каждого кластера и сохранение файлов\n",
        "    thresholds = calculate_and_save_thresholds_with_names(clustered_dfs, cluster_names)\n",
        "\n",
        "    # Инициализируем список для хранения данных по каждому кластеру\n",
        "    cluster_info = []\n",
        "    total_transactions_all_clusters = 0\n",
        "    total_errors_all_clusters = 0\n",
        "\n",
        "    # Рассчитываем ложные срабатывания и добавляем общее количество транзакций для каждого кластера\n",
        "    all_clusters_df = pd.DataFrame()  # Общая таблица для всех кластеров\n",
        "    for i, cluster_name in enumerate(cluster_names):\n",
        "        threshold = thresholds[cluster_name]\n",
        "        df_cluster = clustered_dfs[i]\n",
        "\n",
        "        # Рассчитываем ложные срабатывания и количество транзакций\n",
        "        df_cluster_with_errors = calculate_false_triggers(df_transactions, df_cluster, threshold)\n",
        "\n",
        "        # Собираем информацию для таблицы\n",
        "        total_transactions = df_cluster_with_errors['total_trans'].sum()\n",
        "        total_errors = df_cluster_with_errors['total_errors'].sum()\n",
        "        error_rate = (total_errors / total_transactions) * 100 if total_transactions > 0 else 0\n",
        "        num_rows = df_cluster_with_errors.shape[0]\n",
        "\n",
        "        # Обновляем общий счетчик транзакций и ошибок\n",
        "        total_transactions_all_clusters += total_transactions\n",
        "        total_errors_all_clusters += total_errors\n",
        "\n",
        "        cluster_info.append({\n",
        "            'Кластер': cluster_name,\n",
        "            'Перцентиль 90%': int(round(df_cluster['percentile_90'].quantile(0.9))),\n",
        "            'Порог срабатывания': int(round(threshold)),\n",
        "            'Всего транзакций': int(total_transactions),\n",
        "            'Ложных срабатываний': int(total_errors),\n",
        "            'Процент ложных срабатываний': f\"{error_rate:.2f}%\",\n",
        "            'Кол-во строк': num_rows\n",
        "        })\n",
        "\n",
        "    # Преобразуем собранные данные в DataFrame для вывода в виде таблицы\n",
        "    cluster_info_df = pd.DataFrame(cluster_info)\n",
        "\n",
        "    # Вывод таблицы\n",
        "    print(\"\\nИнформация по кластерам:\")\n",
        "    print(cluster_info_df.to_string(index=False))\n",
        "\n",
        "    # Рассчитываем и выводим общий процент ложных срабатываний по всем кластерам\n",
        "    overall_error_rate = (total_errors_all_clusters / total_transactions_all_clusters) * 100 if total_transactions_all_clusters > 0 else 0\n",
        "\n",
        "    # Вывод общего процента ложных срабатываний по всем кластерам\n",
        "    print(f\"\\nОбщий процент ложных срабатываний по всем кластерам: {overall_error_rate:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8wADaNAON23",
        "outputId": "04050d58-ee5f-4f25-875c-21002b287ca7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Индекс Davies-Bouldin: 1.4923101373565508\n",
            "\n",
            "Информация по кластерам:\n",
            "  Кластер  Перцентиль 90%  Порог срабатывания  Всего транзакций  Ложных срабатываний Процент ложных срабатываний  Кол-во строк\n",
            "cluster_0         1297000             1297000             87038                 3983                       4.58%          2276\n",
            "cluster_1          100000              100000                 7                    0                       0.00%             7\n",
            "\n",
            "Общий процент ложных срабатываний по всем кластерам: 4.58%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Иерархическая кластеризация (дивизонная)"
      ],
      "metadata": {
        "id": "trdekHKJOPkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.cluster.hierarchy import fcluster, linkage\n",
        "from sklearn.metrics import davies_bouldin_score\n",
        "from scipy.spatial.distance import pdist\n",
        "\n",
        "# 1. Загрузка и подготовка данных\n",
        "\n",
        "def load_and_process_csv(csv_file):\n",
        "    \"\"\"\n",
        "    Загружает данные из CSV, проверяет их корректность и выполняет необходимые преобразования.\n",
        "\n",
        "    Аргументы:\n",
        "        csv_file (str): путь к CSV-файлу.\n",
        "\n",
        "    Возвращает:\n",
        "        pd.DataFrame: обработанный датафрейм с рассчитанными процентилями и оборотами.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(csv_file, delimiter=';', header=None)\n",
        "    df.columns = ['inn', 'org_name', 'amount', 'type', 'code', 'timestamp']\n",
        "\n",
        "    # Преобразование суммы в числовой формат\n",
        "    df['amount'] = df['amount'].apply(lambda x: str(x).replace(',', '.'))\n",
        "    df['amount'] = pd.to_numeric(df['amount'], errors='coerce')\n",
        "\n",
        "    # Группировка по INN и расчет percentil 90 и total_payments\n",
        "    result = df.groupby(['inn', 'org_name']).agg(\n",
        "        percentile_90=('amount', lambda x: x.quantile(0.9)),\n",
        "        total_payments=('amount', 'sum')\n",
        "    ).reset_index()\n",
        "\n",
        "    return df, result  # Возвращаем и исходный DataFrame для расчетов ложных срабатываний\n",
        "\n",
        "# 2. Иерархическая дивизионная кластеризация\n",
        "\n",
        "def divisive_clustering(df, metric='euclidean', min_cluster_size=2, n_clusters=3):\n",
        "    \"\"\"\n",
        "    Выполняет иерархическую дивизионную кластеризацию.\n",
        "\n",
        "    Аргументы:\n",
        "        df (pd.DataFrame): датафрейм с данными для кластеризации.\n",
        "        metric (str): метрика расстояния (например, 'euclidean', 'manhattan').\n",
        "        min_cluster_size (int): минимальный размер кластера.\n",
        "        n_clusters (int): целевое количество кластеров.\n",
        "\n",
        "    Возвращает:\n",
        "        list: список датафреймов для каждого кластера.\n",
        "    \"\"\"\n",
        "    # Вычисление матрицы расстояний\n",
        "    distances = pdist(df[['percentile_90', 'total_payments']], metric=metric)\n",
        "\n",
        "    # Применение метода иерархической кластеризации\n",
        "    Z = linkage(distances, method='ward')  # Используем метод Уорда для минимизации внутрикластерной дисперсии\n",
        "\n",
        "    # Получаем кластеры\n",
        "    labels = fcluster(Z, t=n_clusters, criterion='maxclust')\n",
        "    df['n_cluster'] = labels\n",
        "\n",
        "    clusters = []\n",
        "    for cluster_label in np.unique(labels):\n",
        "        cluster_df = df[df['n_cluster'] == cluster_label]\n",
        "        if len(cluster_df) >= min_cluster_size:\n",
        "            clusters.append(cluster_df.copy())\n",
        "\n",
        "    return clusters, labels\n",
        "\n",
        "# 3. Оценка качества кластеризации с помощью Davies-Bouldin Index\n",
        "\n",
        "def evaluate_clustering_with_davies_bouldin(df, cluster_labels):\n",
        "    \"\"\"\n",
        "    Оценивает качество кластеризации с помощью индекса Davies-Bouldin.\n",
        "\n",
        "    Аргументы:\n",
        "        df (pd.DataFrame): датафрейм с данными для оценки.\n",
        "        cluster_labels (array): метки кластеров.\n",
        "\n",
        "    Возвращает:\n",
        "        float: индекс Davies-Bouldin.\n",
        "    \"\"\"\n",
        "    X = df[['percentile_90', 'total_payments']].values\n",
        "    if len(set(cluster_labels)) > 1:\n",
        "        score = davies_bouldin_score(X, cluster_labels)\n",
        "    else:\n",
        "        score = float('inf')  # Если кластер один, Davies-Bouldin не может быть рассчитан\n",
        "    return score\n",
        "\n",
        "# 4. Расчет порогов срабатывания и ложных срабатываний\n",
        "\n",
        "def calculate_false_triggers(df_transactions, df_cluster, threshold):\n",
        "    \"\"\"\n",
        "    Рассчитывает количество ложных срабатываний для каждого ИНН в кластере.\n",
        "\n",
        "    Аргументы:\n",
        "        df_transactions (pd.DataFrame): датафрейм с транзакциями.\n",
        "        df_cluster (pd.DataFrame): датафрейм с данными кластера.\n",
        "        threshold (float): порог для данного кластера.\n",
        "\n",
        "    Возвращает:\n",
        "        pd.DataFrame: датафрейм с добавленными столбцами 'total_errors' и 'total_trans'.\n",
        "    \"\"\"\n",
        "    # Подсчет количества транзакций для каждого ИНН\n",
        "    trans_counts = df_transactions.groupby('inn').agg(total_trans=('amount', 'count')).reset_index()\n",
        "\n",
        "    merged_df = pd.merge(df_transactions, df_cluster[['inn', 'threshold']], on='inn', how='left')\n",
        "\n",
        "    # Рассчет ошибок на основе транзакций\n",
        "    merged_df['errors'] = merged_df['amount'] > threshold\n",
        "    false_triggers_df = merged_df.groupby('inn').agg(total_errors=('errors', 'sum')).reset_index()\n",
        "\n",
        "    # Объединяем обратно с кластером\n",
        "    df_cluster = pd.merge(df_cluster, false_triggers_df[['inn', 'total_errors']], on='inn', how='left')\n",
        "    df_cluster = pd.merge(df_cluster, trans_counts[['inn', 'total_trans']], on='inn', how='left')\n",
        "\n",
        "    # Заполняем возможные пропуски (NaN) нулями\n",
        "    df_cluster['total_errors'] = df_cluster['total_errors'].fillna(0)\n",
        "    df_cluster['total_trans'] = df_cluster['total_trans'].fillna(0)\n",
        "\n",
        "    return df_cluster\n",
        "\n",
        "# 5. Основная часть кода\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Загрузка данных\n",
        "    file_path = 'fraud_02.2024.csv'\n",
        "    df_transactions, df_aggregated = load_and_process_csv(file_path)\n",
        "\n",
        "    # Применение иерархической дивизионной кластеризации\n",
        "    metric = 'euclidean'  # Выбор метрики расстояния\n",
        "    min_cluster_size = 5  # Минимальный размер кластера\n",
        "    n_clusters = 5  # Целевое количество кластеров\n",
        "    clustered_dfs, cluster_labels = divisive_clustering(df_aggregated, metric=metric, min_cluster_size=min_cluster_size, n_clusters=n_clusters)\n",
        "\n",
        "    # Оценка индекса Davies-Bouldin\n",
        "    davies_bouldin_score_value = evaluate_clustering_with_davies_bouldin(df_aggregated, cluster_labels)\n",
        "    print(f\"Индекс Davies-Bouldin: {davies_bouldin_score_value}\")\n",
        "\n",
        "    # Расчет ложных срабатываний для каждого кластера\n",
        "    cluster_info = []\n",
        "    total_transactions_all_clusters = 0\n",
        "    total_errors_all_clusters = 0\n",
        "\n",
        "    for i, df_cluster in enumerate(clustered_dfs):\n",
        "        # Определяем порог срабатывания и добавляем его в DataFrame\n",
        "        threshold = df_cluster['percentile_90'].quantile(0.9)\n",
        "        df_cluster['threshold'] = threshold  # Добавляем порог в DataFrame\n",
        "\n",
        "        # Рассчитываем ложные срабатывания\n",
        "        df_cluster_with_errors = calculate_false_triggers(df_transactions, df_cluster, threshold)\n",
        "\n",
        "        total_transactions = df_cluster_with_errors['total_trans'].sum()\n",
        "        total_errors = df_cluster_with_errors['total_errors'].sum()\n",
        "        error_rate = (total_errors / total_transactions) * 100 if total_transactions > 0 else 0\n",
        "        num_rows = df_cluster_with_errors.shape[0]\n",
        "\n",
        "        total_transactions_all_clusters += total_transactions\n",
        "        total_errors_all_clusters += total_errors\n",
        "\n",
        "        cluster_info.append({\n",
        "            'Кластер': f'cluster_{i}',\n",
        "            'Перцентиль 90%': int(round(threshold)),\n",
        "            'Порог срабатывания': int(round(threshold)),\n",
        "            'Всего транзакций': int(total_transactions),\n",
        "            'Ложных срабатываний': int(total_errors),\n",
        "            'Процент ложных срабатываний': f\"{error_rate:.2f}%\",\n",
        "            'Кол-во строк': num_rows\n",
        "        })\n",
        "\n",
        "        # Сохранение каждого кластера в отдельный Excel-файл\n",
        "        file_name = f'cluster_{i}_data_with_errors.xlsx'\n",
        "        df_cluster_with_errors.to_excel(file_name, index=False)\n",
        "        print(f\"Файл для кластера '{i}' сохранен как {file_name}\")\n",
        "\n",
        "    # Преобразуем собранные данные в DataFrame для вывода в виде таблицы\n",
        "    cluster_info_df = pd.DataFrame(cluster_info)\n",
        "\n",
        "    # Вывод информации по кластерам\n",
        "    print(\"\\nИнформация по кластерам:\")\n",
        "    print(cluster_info_df.to_string(index=False))\n",
        "\n",
        "    # Вывод общего процента ложных срабатываний\n",
        "    overall_error_rate = (total_errors_all_clusters / total_transactions_all_clusters) * 100 if total_transactions_all_clusters > 0 else 0\n",
        "    print(f\"\\nОбщий процент ложных срабатываний по всем кластерам: {overall_error_rate:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKHAW5DCIhW7",
        "outputId": "8954d27d-d445-4639-a78b-b61b2c4eae1e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Индекс Davies-Bouldin: 0.37424250455051966\n",
            "Файл для кластера '0' сохранен как cluster_0_data_with_errors.xlsx\n",
            "Файл для кластера '1' сохранен как cluster_1_data_with_errors.xlsx\n",
            "Файл для кластера '2' сохранен как cluster_2_data_with_errors.xlsx\n",
            "\n",
            "Информация по кластерам:\n",
            "  Кластер  Перцентиль 90%  Порог срабатывания  Всего транзакций  Ложных срабатываний Процент ложных срабатываний  Кол-во строк\n",
            "cluster_0          737700              737700             53745                 1477                       2.75%          2092\n",
            "cluster_1         7364975             7364975             19547                  154                       0.79%           146\n",
            "cluster_2        23940000            23940000             12352                   47                       0.38%            41\n",
            "\n",
            "Общий процент ложных срабатываний по всем кластерам: 1.96%\n"
          ]
        }
      ]
    }
  ]
}